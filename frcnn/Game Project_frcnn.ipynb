{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46bd678d-cb7c-4419-8e26-e9c6e5b9236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from mss import mss\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from dataset import get_transforms\n",
    "\n",
    "import pyautogui\n",
    "import time\n",
    "import win32api, win32con\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "LOAD_MODEL_FILE = \"FRCNN_halo_FRCNN_model.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded checkpoint at epoch 26\n"
     ]
    },
    {
     "data": {
      "text/plain": "FasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (layer_blocks): ModuleList(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHECKPOINT_NAME = LOAD_MODEL_FILE\n",
    "checkpoint = torch.load(CHECKPOINT_NAME)\n",
    "\n",
    "transform = get_transforms(train=False)\n",
    "\n",
    "# load a pre-trained pre-trained FRCNN from torch\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one using our one enemy class.  Use 2 here because we need a background class\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "print(f'loaded checkpoint at epoch {epoch}')\n",
    "\n",
    "model.eval()\n",
    "model.to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
    "\n",
    "    # torchvision returns the indices of the bboxes to keep\n",
    "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
    "\n",
    "    final_prediction = orig_prediction\n",
    "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
    "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
    "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
    "\n",
    "    return final_prediction\n",
    "\n",
    "# function to convert a torchtensor back to PIL image\n",
    "def torch_to_pil(img):\n",
    "    return torchvision.transforms.ToPILImage()(img).convert('RGB')\n",
    "\n",
    "import tkinter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def plot_img_bbox(img, target):\n",
    "    # plot the image and bboxes\n",
    "    # Bounding boxes are defined as follows: x-min y-min width height\n",
    "    fig, a = plt.subplots(1,1)\n",
    "    fig.set_size_inches(5,5)\n",
    "    a.imshow(img)\n",
    "    for box in (target['boxes']):\n",
    "        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
    "        rect = patches.Rectangle((x, y),\n",
    "                                 width, height,\n",
    "                                 linewidth = 2,\n",
    "                                 edgecolor = 'r',\n",
    "                                 facecolor = 'none')\n",
    "\n",
    "        # Draw the bounding box on top of the image\n",
    "        a.add_patch(rect)\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "model.rpn.score_thresh = .5\n",
    "model.rpn._pre_nms_top_n = {'training': 500, 'testing': 500}\n",
    "model.rpn._post_nms_top_n = {'training': 250, 'testing': 250}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "show = True\n",
    "dbg = True\n",
    "\n",
    "from YOLOmodel import utils\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "#centering is how much of the screen to account for, from the center\n",
    "centering = 1\n",
    "threshold = .8\n",
    "sct = mss()\n",
    "\n",
    "movement_scale = 1.8\n",
    "\n",
    "w, h = pyautogui.size()\n",
    "\n",
    "x_center = w/2\n",
    "y_center = h/2\n",
    "\n",
    "top = int(y_center - (y_center * centering))\n",
    "left = int(x_center - (x_center * centering))\n",
    "bottom = int(y_center + (y_center * centering))\n",
    "right = int(x_center + (x_center * centering))\n",
    "\n",
    "width = int(w * centering)\n",
    "height = int(h * centering)\n",
    "monitor = {'top': top, 'left': left, 'width': width, 'height': height}\n",
    "resize = (448, 448)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "import pyautogui\n",
    "\n",
    "while True:\n",
    "    if dbg:\n",
    "        start = time.time()\n",
    "\n",
    "    orig_img = sct.grab(monitor)\n",
    "    img = Image.frombytes('RGB', (orig_img.size.width, orig_img.size.height), orig_img.rgb)\n",
    "    img = np.array(img)\n",
    "\n",
    "\n",
    "    img = np.array(img).transpose(2,0,1)\n",
    "\n",
    "    img = torch.tensor(img).to(DEVICE)\n",
    "\n",
    "    img = img / 255.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(DEVICE)])[0]\n",
    "\n",
    "    cpu_pred = {}\n",
    "    for key, value in prediction.items():\n",
    "        cpu_pred[key] = value.cpu()\n",
    "\n",
    "    img = np.array(torch_to_pil(img))\n",
    "\n",
    "    if dbg:\n",
    "        framerate = 1/(time.time() - start)\n",
    "\n",
    "    #add framerate to the cv2 image\n",
    "    cv2.putText(img, str(framerate), (50, 50), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                   1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    for i, box in enumerate(cpu_pred['boxes']):\n",
    "        if cpu_pred['scores'][i] > threshold:\n",
    "            box_dim = (box[3] - box[1])  #also used to scale inputs later on\n",
    "            x = (box[2] + box[0]) / 2\n",
    "            y = ((box[3] + box[1]) / 2) - ((box[3] - box[1]) / 3)\n",
    "            cv2.circle(img,\n",
    "                (int(x), int(y)),\n",
    "                radius=10,\n",
    "                color=(255, 0, 0),\n",
    "                thickness=5\n",
    "            )\n",
    "\n",
    "            win32api.mouse_event(win32con.MOUSEEVENTF_MOVE, int(movement_scale*box_dim/orig_img.size.height*(x.item()-x_center)), int(movement_scale*box_dim/orig_img.size.height*(y.item()-y_center)))\n",
    "\n",
    "    if show:\n",
    "        cv2.imshow('test', cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        print(framerate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# # to view test images\n",
    "#\n",
    "# import glob\n",
    "# # for img in glob.glob('C:/Users/blain/Documents\\Git\\AutoAim\\model\\halo_data/*.png'):\n",
    "# for img in glob.glob('C:/Users/blain\\Documents\\Git\\AutoAim\\model\\halo_data/new/*'):\n",
    "#\n",
    "#     img = Image.open(img)\n",
    "#     img = img.resize(resize)\n",
    "#     img_transform = transform(img).unsqueeze(0).cuda()\n",
    "#     img = np.array(img)\n",
    "#\n",
    "#\n",
    "#     boxes = utils.cellboxes_to_boxes(model(img_transform))\n",
    "#     boxes = utils.non_max_suppression(boxes[0], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "#     primary, secondary = utils.pick_targets(img, boxes)\n",
    "#\n",
    "#     # add aim points to image\n",
    "#     for aimpoint in primary:\n",
    "#         print(aimpoint)\n",
    "#         cv2.circle(img,\n",
    "#             (int(aimpoint[0]), int(aimpoint[1])),\n",
    "#             radius=10,\n",
    "#             color=(255, 0, 0),\n",
    "#             thickness=5\n",
    "#         )\n",
    "#     cv2.imshow('test', cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n",
    "#\n",
    "#     if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "#\n",
    "#         break\n",
    "# cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "autoaim",
   "language": "python",
   "display_name": "autoaim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}